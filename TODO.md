## TODO

Legend:
- ğŸ”´ not started
- ğŸŸ¡ in progress
- ğŸŸ¢ done

---

### 0) Project setup
- ğŸ”´ Choose a wiki (not wikipedia.org) and verify its license/terms allow scraping
- ğŸ”´ Set base URL and article link prefix (e.g., `/wiki/`)
- ğŸ”´ Create virtual environment and `requirements.txt`
- ğŸ”´ Add `.gitignore` (venv, cache, pycache, etc.)
- ğŸ”´ Initialize git repo and make regular commits

---

### 1) Core architecture (importable modules)
- ğŸ”´ Implement `models.py` (`Page`, traversal `Node/State`)
- ğŸ”´ Implement `cli.py` argument parsing (without `argparse` if required)
- ğŸ”´ Implement `controller.py` (main controller class, receives parsed config)
- ğŸ”´ Implement `scraper.py` (network fetch + optional local HTML mode)
- ğŸ”´ Implement `html_parser.py` (extract content-only text, links, tables)
- ğŸ”´ Implement `cache.py` (disk cache read/write)
- ğŸ”´ Implement `text_utils.py` (tokenize, normalize, count_words, summary)
- ğŸ”´ Implement `output.py` (pretty console/table output)
- ğŸ”´ Create `wiki_scraper.py` entry point (calls CLI + controller)

---

### 2) CLI commands (functional requirements)
- ğŸ”´ `--summary "phrase"`: print first paragraph text (no HTML)
- ğŸ”´ Handle missing article / not found cases gracefully

- ğŸ”´ `--table "phrase" --number n [--first-row-is-header]`
  - ğŸ”´ Extract nth `<table>` and convert to pandas DataFrame
  - ğŸ”´ Save to `"phrase.csv"`
  - ğŸ”´ Print value counts table (excluding headers)

- ğŸ”´ `--count-words "phrase"`
  - ğŸ”´ Extract full article text (no menus/sidebars)
  - ğŸ”´ Update cumulative `./word-counts.json`

- ğŸ”´ `--analyze-relative-word-frequency --mode {article|language} --count n [--chart path]`
  - ğŸ”´ Load top 1000+ language word frequencies (e.g., `wordfreq`)
  - ğŸ”´ Build DataFrame: word, freq_in_article_norm, freq_in_language_norm
  - ğŸ”´ Print DataFrame (pandas-style)
  - ğŸ”´ Optional: save bar chart image with legend and title

- ğŸ”´ `--auto-count-words "start" --depth n --wait t`
  - ğŸ”´ Traverse wiki graph (BFS or DFS)
  - ğŸ”´ Track visited pages
  - ğŸ”´ Respect depth limit and delay between requests
  - ğŸ”´ Reuse `--count-words` logic for each visited page

---

### 3) Testing (offline)
- ğŸ”´ Add `tests/data/` with at least one saved HTML page (e.g., `team_rocket.html`)

- ğŸ”´ Write 4 unit tests (no internet):
  - ğŸ”´ `is_article_link(href)` correctly classifies links
  - ğŸ”´ `phrase_to_url("Team Rocket")` builds correct URL
  - ğŸ”´ `extract_first_paragraph(html)` returns expected start/end
  - ğŸ”´ `count_words("a a b")` returns correct counts

- ğŸ”´ Integration test program:
  - ğŸ”´ Create `tests/wiki_scraper_integration_test.py`
  - ğŸ”´ Ensure it runs via `python wiki_scraper_integration_test.py`
  - ğŸ”´ Load HTML from disk (no network)
  - ğŸ”´ Test one main feature (e.g., summary) using assertions
  - ğŸ”´ Exit with non-zero code on failure

---

### 4) Language detection analysis (Jupyter)
- ğŸ”´ Define `lang_confidence_score(word_counts, language_words_with_frequency)`
- ğŸ”´ Collect language frequency lists (â‰¥ 1000 words) for 3 languages
- ğŸ”´ Prepare test datasets:
  - ğŸ”´ Long wiki article (â‰¥ 5000 words) via `word-counts.json`
  - ğŸ”´ Short wiki article (â‰¥ 20 words) minimizing score for wiki language
  - ğŸ”´ Long non-wiki text for each of the 3 languages

- ğŸ”´ Evaluate for k = 3, 10, 100, 1000 across:
  - ğŸ”´ 3 languages Ã— 5 texts

- ğŸ”´ Create clear plots for results
- ğŸ”´ Write conclusions answering:
  - ğŸ”´ Does language choice matter?
  - ğŸ”´ Do frequencies show inflection-rich language behavior?
  - ğŸ”´ Was it hard to find a low-score wiki article and why?

---

### 5) Final polish
- ğŸ”´ Ensure code is PEP8-compliant (â‰¤ 3 intentional deviations allowed)
- ğŸ”´ Ensure modules are importable in REPL/Jupyter
- ğŸ”´ Update README with usage examples for each CLI command
- ğŸ”´ Confirm `cache/` is ignored by git, but `tests/data/` is committed
- ğŸ”´ Dry-run presentation: explain module responsibilities and used libraries
